{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c804b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bf1f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40aacdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow.keras as keras\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cb26a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial environment\n",
    "env = gym.make('PongNoFrameskip-v4', render_mode='human')\n",
    "\n",
    "#Atari preprocessing wrapper\n",
    "env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
    "#Frame stacking\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10da83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state\n",
    "state_size = (88, 80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a240d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of actions \n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83ea14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the game screen\n",
    "\n",
    "def preprocess_state(state):\n",
    "    # Assuming state is a tuple, and the first element is the game screen\n",
    "    game_screen = state[0]\n",
    "    game_screen = np.array(game_screen)\n",
    "    # Crop and resize the image\n",
    "    image = game_screen[1:176:2, ::2]\n",
    "\n",
    "    # Convert the image to greyscale\n",
    "    image = image.mean(axis=2)\n",
    "\n",
    "    # Improve image contrast\n",
    "    color = image.mean()\n",
    "    image[image == color] = 0\n",
    "\n",
    "    # Normalize the image\n",
    "    image = (image - 128) / 128 - 1\n",
    "\n",
    "    # Reshape the image\n",
    "    #image = np.expand_dims(image.reshape(88, 80, 1), axis=0)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b3f59",
   "metadata": {},
   "source": [
    "## Building the Deep Q Networks  \n",
    "\n",
    "For playing atari games we use the CNN as the DQN which takes the image of the game screen as an input and returns the Q values.\n",
    "\n",
    "Defining the DQN with three convolutional layers. \n",
    "\n",
    "1) The convolutional layers extract the features from the image and output the feature maps\n",
    "\n",
    "2) We flattened the feature map obtained by the convolutional layers \n",
    "\n",
    "3) Feeding the flattened feature maps to the feedforward network (fully connected layer) which returns the Q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4981335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        #define the state size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        #define the action size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #define the replay buffer\n",
    "        self.replay_buffer = deque(maxlen=5000)\n",
    "        \n",
    "        #define the discount factor\n",
    "        self.gamma = 0.9  \n",
    "        \n",
    "        #define the epsilon value\n",
    "        self.epsilon = 0.8   \n",
    "        \n",
    "        #define the update rate at which we want to update the target network\n",
    "        self.update_rate = 1000    \n",
    "        \n",
    "        #define the main network\n",
    "        self.main_network = self.build_network()\n",
    "        \n",
    "        #define the target network\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        #copy the weights of the main network to the target network\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "\n",
    "    #Let's define a function called build_network which is essentially our DQN. \n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "        return model\n",
    "    \n",
    "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the replay buffer. \n",
    "    #So, we define a function called store_transition which stores the transition information into the replay buffer\n",
    "\n",
    "    def store_transistion(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
    "    \n",
    "    def epsilon_greedy(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        Q_values = self.main_network.predict(state)\n",
    "        \n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "    \n",
    "    #train the network\n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        #sample a mini batch of transition from the replay buffer\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        \n",
    "        #compute the Q value using the target network\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target_Q = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "            else:\n",
    "                target_Q = reward\n",
    "                \n",
    "            #compute the Q value using the main network \n",
    "            Q_values = self.main_network.predict(state)\n",
    "            \n",
    "            Q_values[0][action] = target_Q\n",
    "            \n",
    "            #train the main network\n",
    "            self.main_network.fit(state, Q_values, epochs=1, verbose=0)\n",
    "            \n",
    "    #update the target network weights by copying from the main network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1f814",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a189ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f62285e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "413b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc0dc921",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_screens = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f39f229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8aefc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99783213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "time_step = 0\n",
    "\n",
    "#for each episode\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #set return to 0\n",
    "    Return = 0\n",
    "    \n",
    "    #preprocess the game screen\n",
    "    state = preprocess_state(env.reset())\n",
    "\n",
    "    #for each step in the episode\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        #render the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        state = env.render()\n",
    "        \n",
    "        #update the time step\n",
    "        time_step += 1\n",
    "        \n",
    "        #update the target network\n",
    "        if time_step % dqn.update_rate == 0:\n",
    "            dqn.update_target_network()\n",
    "        \n",
    "        #select the action\n",
    "        action = dqn.epsilon_greedy(state)\n",
    "        \n",
    "        #perform the selected action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        #preprocess the next state\n",
    "        next_state = preprocess_state(next_state)\n",
    "        \n",
    "        #store the transition information\n",
    "        dqn.store_transistion(state, action, reward, next_state, done)\n",
    "        \n",
    "        #update current state to next state\n",
    "        state = next_state\n",
    "        \n",
    "        #update the return\n",
    "        Return += reward\n",
    "        \n",
    "        #if the episode is done then print the return\n",
    "        if done:\n",
    "            print('Episode: ',i, ',' 'Return', Return)\n",
    "            break\n",
    "            \n",
    "        #if the number of transistions in the replay buffer is greater than batch size\n",
    "        #then train the network\n",
    "        if len(dqn.replay_buffer) > batch_size:\n",
    "            dqn.train(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7dc18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
